"""
YouTube è®¢é˜…é•¿è§†é¢‘æ‘˜è¦ â†’ é£ä¹¦æ¨é€
- RSS è½®è¯¢è®¢é˜…é¢‘é“æ–°è§†é¢‘ï¼ˆRSS å¤©ç„¶ä¸å« Shortsï¼‰
- LLM æ™ºèƒ½ç­›é€‰æœ€å€¼å¾—æ·±åº¦è§‚çœ‹çš„è§†é¢‘
- ç”Ÿæˆæ‘˜è¦ï¼Œæ¨é€åˆ°é£ä¹¦ï¼ˆå¼€æ”¾å¹³å°åº”ç”¨ï¼‰
"""

import os
import re
import json
import time
import requests
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from pathlib import Path

# ============ é…ç½® ============
FEISHU_APP_ID = os.environ.get("FEISHU_APP_ID", "")
FEISHU_APP_SECRET = os.environ.get("FEISHU_APP_SECRET", "")
FEISHU_USER_ID = os.environ.get("FEISHU_USER_ID", "")  # ç›®æ ‡ç”¨æˆ· ID (ou_xxxxx)
FEISHU_WEBHOOK_URL = os.environ.get("FEISHU_WEBHOOK_URL", "")  # ç¾¤æœºå™¨äºº Webhook
MINIMAX_API_KEY = os.environ.get("MINIMAX_API_KEY", "")
MINIMAX_API_BASE = os.environ.get("MINIMAX_API_BASE", "https://api.minimaxi.com/anthropic")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "")
MIN_DURATION_MINUTES = int(os.environ.get("MIN_DURATION_MINUTES", "3"))  # è¿‡æ»¤ Shortsï¼ˆ<=3minï¼‰
TOP_N = int(os.environ.get("TOP_N", "5"))  # æ¯æ—¥æ¨é€ Top N è§†é¢‘
LOOKBACK_HOURS = int(os.environ.get("LOOKBACK_HOURS", "24"))
CHANNELS_FILE = os.environ.get("CHANNELS_FILE", "channels.json")
PROFILE_FILE = os.environ.get("PROFILE_FILE", "profile.json")
HISTORY_FILE = os.environ.get("HISTORY_FILE", "history.json")
HISTORY_MAX_DAYS = int(os.environ.get("HISTORY_MAX_DAYS", "30"))


def load_channels() -> list[dict]:
    """åŠ è½½é¢‘é“åˆ—è¡¨"""
    path = Path(CHANNELS_FILE)
    if not path.exists():
        print(f"âŒ {CHANNELS_FILE} not found")
        return []
    with open(path) as f:
        return json.load(f)


def load_profile() -> dict:
    """åŠ è½½ç”¨æˆ·ç”»åƒé…ç½®"""
    path = Path(PROFILE_FILE)
    if not path.exists():
        print(f"âš ï¸ {PROFILE_FILE} not found, using defaults")
        return {
            "description": "ç§‘æŠ€è¡Œä¸šä»ä¸šè€…",
            "favorite_content": "æ·±åº¦è®¿è°ˆã€æŠ€æœ¯åˆ†äº«",
            "preferred_channels": [],
            "exclude_title_patterns": ["full course", "tutorial for beginners"],
        }
    with open(path) as f:
        return json.load(f)


def load_history() -> dict:
    """åŠ è½½å·²å¤„ç†è§†é¢‘ ID â†’ æ—¶é—´æˆ³æ˜ å°„ï¼Œé¿å…é‡å¤æ¨é€"""
    path = Path(HISTORY_FILE)
    if not path.exists():
        return {}
    with open(path) as f:
        data = json.load(f)
    # å…¼å®¹æ—§æ ¼å¼ï¼ˆçº¯åˆ—è¡¨ï¼‰
    if isinstance(data, list):
        now = datetime.now(timezone.utc).isoformat()
        return {vid: now for vid in data}
    return data


def save_history(history: dict):
    """ä¿å­˜å†å²è®°å½•ï¼Œè‡ªåŠ¨æ¸…ç†è¶…è¿‡ HISTORY_MAX_DAYS çš„æ¡ç›®"""
    cutoff = (datetime.now(timezone.utc) - timedelta(days=HISTORY_MAX_DAYS)).isoformat()
    cleaned = {vid: ts for vid, ts in history.items() if ts > cutoff}
    if len(cleaned) < len(history):
        print(f"  ğŸ§¹ æ¸…ç†å†å²è®°å½•: {len(history)} â†’ {len(cleaned)} æ¡")
    path = Path(HISTORY_FILE)
    with open(path, "w") as f:
        json.dump(cleaned, f)


# ============ YouTube RSS ============
def fetch_rss_videos(channel_id: str) -> list[dict]:
    """ä» YouTube RSS è·å–é¢‘é“æœ€æ–°è§†é¢‘"""
    url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print(f"  âš ï¸ RSS fetch failed for {channel_id}: {e}")
        return []

    ns = {
        "atom": "http://www.w3.org/2005/Atom",
        "yt": "http://www.youtube.com/xml/schemas/2015",
        "media": "http://search.yahoo.com/mrss/",
    }
    root = ET.fromstring(resp.text)
    cutoff = datetime.now(timezone.utc) - timedelta(hours=LOOKBACK_HOURS)
    videos = []

    for entry in root.findall("atom:entry", ns):
        published_str = entry.find("atom:published", ns).text
        published = datetime.fromisoformat(published_str.replace("Z", "+00:00"))
        if published < cutoff:
            continue

        video_id = entry.find("yt:videoId", ns).text
        title = entry.find("atom:title", ns).text
        author = root.find("atom:title", ns).text

        videos.append({
            "video_id": video_id,
            "title": title,
            "author": author,
            "published": published_str,
            "url": f"https://www.youtube.com/watch?v={video_id}",
        })

    return videos


# ============ YouTube Data API (è§†é¢‘æ—¶é•¿) ============
def parse_duration(iso_duration: str) -> int:
    """ISO 8601 duration â†’ ç§’æ•°ï¼Œä¾‹å¦‚ PT1H23M45S"""
    match = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", iso_duration)
    if not match:
        return 0
    h = int(match.group(1) or 0)
    m = int(match.group(2) or 0)
    s = int(match.group(3) or 0)
    return h * 3600 + m * 60 + s


def get_video_details(video_id: str) -> dict:
    """é€šè¿‡ YouTube Data API è·å–è§†é¢‘æ—¶é•¿ã€æè¿°ã€æ’­æ”¾é‡"""
    if not YOUTUBE_API_KEY:
        print("  âš ï¸ No YOUTUBE_API_KEY, skipping details fetch")
        return {"duration": 9999, "description": "", "view_count": 0}
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {"part": "contentDetails,snippet,statistics", "id": video_id, "key": YOUTUBE_API_KEY}
    try:
        resp = requests.get(url, params=params, timeout=10)
        data = resp.json()
        item = data["items"][0]
        duration_str = item["contentDetails"]["duration"]
        description = item["snippet"].get("description", "")
        view_count = int(item["statistics"].get("viewCount", 0))
        return {
            "duration": parse_duration(duration_str),
            "description": description,
            "view_count": view_count,
        }
    except Exception as e:
        print(f"  âš ï¸ Details fetch failed: {e}")
        return {"duration": 0, "description": "", "view_count": 0}


def format_duration(seconds: int) -> str:
    h, remainder = divmod(seconds, 3600)
    m, s = divmod(remainder, 60)
    if h > 0:
        return f"{h}h{m:02d}m"
    return f"{m}m{s:02d}s"


_yt_cookies_file = os.environ.get("YT_COOKIES_FILE", "")


def get_transcript(video_id: str) -> str | None:
    """é€šè¿‡ yt-dlp è·å–è§†é¢‘å­—å¹•æ–‡æœ¬ï¼ˆä¼˜å…ˆæ‰‹åŠ¨å­—å¹•ï¼Œå…¶æ¬¡è‡ªåŠ¨ç”Ÿæˆï¼‰"""
    try:
        import yt_dlp
        ydl_opts = {
            'skip_download': True,
            'writesubtitles': True,
            'writeautomaticsub': True,
            'subtitleslangs': ['en'],
            'quiet': True,
            'no_warnings': True,
            'ignore_no_formats_error': True,
            'remote_components': {'ejs': 'github'},
        }
        # æ”¯æŒ cookiesï¼šç¯å¢ƒå˜é‡æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼Œæˆ–æœ¬åœ°è‡ªåŠ¨è¯» Chrome
        if _yt_cookies_file and os.path.exists(_yt_cookies_file):
            ydl_opts['cookiefile'] = _yt_cookies_file
        else:
            ydl_opts['cookiesfrombrowser'] = ('chrome',)

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(
                f'https://www.youtube.com/watch?v={video_id}', download=False
            )
            subs = info.get('subtitles', {})
            auto_subs = info.get('automatic_captions', {})
            en_subs = subs.get('en') or auto_subs.get('en')
            if not en_subs:
                return None
            for fmt in en_subs:
                if fmt.get('ext') == 'json3':
                    resp = requests.get(fmt['url'], timeout=15)
                    data = resp.json()
                    texts = []
                    for e in data.get('events', []):
                        for s in e.get('segs', []):
                            t = s.get('utf8', '').strip()
                            if t and t != '\n':
                                texts.append(t)
                    text = ' '.join(texts)
                    if len(text) > 80000:
                        text = text[:80000] + ' ...[truncated]'
                    return text if len(text) > 100 else None
        return None
    except Exception as e:
        print(f"      âš ï¸ å­—å¹•è·å–å¤±è´¥: {e}")
        return None


# ============ Minimax æ‘˜è¦ ============
def summarize_with_llm(title: str, author: str, content: str, content_type: str = "å­—å¹•") -> dict:
    """åŸºäºå­—å¹•æˆ–æè¿°ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦"""
    if not MINIMAX_API_KEY:
        return {"summary": "âš ï¸ æœªé…ç½® MINIMAX_API_KEYï¼Œè·³è¿‡æ‘˜è¦"}

    if len(content) > 80000:
        content = content[:80000] + "\n...[truncated]"

    prompt = f"""æ ¹æ®ä»¥ä¸‹è§†é¢‘{content_type}ï¼Œç”Ÿæˆç®€æ´çš„ä¸­æ–‡æ‘˜è¦ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{title}
é¢‘é“ï¼š{author}

è§†é¢‘{content_type}ï¼š
{content}

æ ¼å¼è¦æ±‚ï¼ˆçº¯æ–‡æœ¬ï¼Œä¸è¦ markdownï¼‰ï¼š
- å¼€å¤´ä¸€æ®µè¯æ¦‚æ‹¬æ ¸å¿ƒå†…å®¹ï¼Œç‚¹æ˜å˜‰å®¾èº«ä»½å’Œè®¨è®ºä¸»é¢˜
- ç”¨ï¼ˆ1ï¼‰ï¼ˆ2ï¼‰ï¼ˆ3ï¼‰ç¼–å·åˆ—å‡º 3-6 ä¸ªè¦ç‚¹ï¼Œå†’å·å‰æ˜¯å…·ä½“å…³é”®è¯æˆ–æ¦‚å¿µåï¼ˆå¦‚"ä¸‰æœˆæ³•åˆ™"ã€"æŠ•èµ„æœºé‡"ã€"èŒƒå¼è½¬ç§»"ï¼‰ï¼Œå†’å·åä¸€å¥è¯æç‚¼æ ¸å¿ƒä¿¡æ¯
- è¦ç‚¹å¿…é¡»æ˜¯å®è´¨æ€§è§‚ç‚¹å’Œå…·ä½“æ´å¯Ÿï¼Œä¸è¦ç©ºæ³›æè¿°
- ç»“å°¾ä¸€å¥æ¨èè¯­ï¼Œè¯´æ˜é€‚åˆè°çœ‹ã€èƒ½è·å¾—ä»€ä¹ˆå¯å‘
- ä¸è¦å‡ºç°"ä¸€å¥è¯æ€»ç»“"ã€"å…³é”®è¦ç‚¹"ã€"æ€»ç»“"ç­‰æ ¼å¼æ ‡ç­¾"""

    result = call_llm(prompt)
    if result:
        return {"summary": result}
    return {"summary": "æ‘˜è¦ç”Ÿæˆå¤±è´¥"}


def call_llm(prompt: str, max_tokens: int = 1024) -> str | None:
    """è°ƒç”¨ Minimax LLMï¼Œè¿”å›æ–‡æœ¬ç»“æœ"""
    if not MINIMAX_API_KEY:
        return None
    try:
        resp = requests.post(
            f"{MINIMAX_API_BASE}/v1/messages",
            headers={
                "x-api-key": MINIMAX_API_KEY,
                "content-type": "application/json",
                "anthropic-version": "2023-06-01",
            },
            json={
                "model": "MiniMax-M2.1",
                "max_tokens": max_tokens,
                "messages": [{"role": "user", "content": prompt}],
            },
            timeout=60,
        )
        data = resp.json()
        if data.get("type") == "error":
            print(f"  âš ï¸ LLM error: {data.get('error', {}).get('message', str(data))}")
            return None
        # ä» content æ•°ç»„ä¸­æå–æœ€åä¸€ä¸ª text block
        for block in reversed(data.get("content", [])):
            if isinstance(block, dict) and block.get("type") == "text":
                return block.get("text", "")
        # fallback: å°è¯•ç›´æ¥å–ç¬¬ä¸€ä¸ª block
        content = data.get("content", [])
        if content and isinstance(content[0], dict):
            return content[0].get("text", str(content[0]))
        return None
    except Exception as e:
        print(f"  âš ï¸ LLM call failed: {e}")
        return None


def call_gemini(prompt: str) -> str | None:
    """è°ƒç”¨ Gemini æ¨¡å‹ï¼Œç”¨äºæ’åºä»»åŠ¡"""
    if not GEMINI_API_KEY:
        return None
    try:
        from google import genai
        client = genai.Client(api_key=GEMINI_API_KEY)
        response = client.models.generate_content(
            model="gemini-3-flash-preview",
            contents=prompt,
        )
        return response.text
    except Exception as e:
        print(f"  âš ï¸ Gemini call failed: {e}")
        return None


def rank_candidates(candidates: list[dict], top_n: int, profile: dict) -> list[dict]:
    """ç”¨ LLM ä»å€™é€‰è§†é¢‘ä¸­æŒ‘é€‰æœ€å€¼å¾—æ·±åº¦è§‚çœ‹çš„ Top Nï¼Œè¿”å› [{index, reason}]"""
    video_list = []
    for i, v in enumerate(candidates):
        desc_snippet = (v.get("description") or "")[:300].replace("\n", " ").strip()
        if desc_snippet:
            desc_snippet = f"\n   æè¿°: {desc_snippet}"
        video_list.append(
            f"{i+1}. [{v['author']}] {v['title']} ({v['duration_str']}, {format_view_count(v['view_count'])} views){desc_snippet}"
        )

    preferred = ", ".join(profile.get("preferred_channels", []))

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†é¢‘ç­›é€‰åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†ç­›é€‰ã€‚

ç”¨æˆ·ç”»åƒï¼š
- {profile.get("description", "ç§‘æŠ€è¡Œä¸šä»ä¸šè€…")}
- å¸¸çœ‹é¢‘é“ï¼š{preferred}
- æœ€å–œæ¬¢çš„å†…å®¹ç±»å‹ï¼š{profile.get("favorite_content", "æ·±åº¦è®¿è°ˆã€æŠ€æœ¯åˆ†äº«")}

ä»¥ä¸‹æ˜¯ä»Šå¤©çš„ {len(candidates)} ä¸ªå€™é€‰è§†é¢‘ï¼š

{chr(10).join(video_list)}

è¯·ä»ä¸­é€‰å‡ºæœ€å€¼å¾—æ·±åº¦è§‚çœ‹çš„ {top_n} ä¸ªè§†é¢‘ã€‚

å¿…é¡»ä¼˜å…ˆé€‰æ‹©ï¼š
1. æœ‰æ·±åº¦çš„ä¸€å¯¹ä¸€è®¿è°ˆæˆ–åœ†æ¡Œè®¨è®ºï¼ˆåˆ›å§‹äººã€ç ”ç©¶è€…ã€æŠ•èµ„äººçš„ä¸€æ‰‹è§‚ç‚¹ï¼‰
2. è¡Œä¸šå¤§ä¼šçš„ä¸»é¢˜æ¼”è®²æˆ–æŠ€æœ¯åˆ†äº«
3. å¯¹ AI æŠ€æœ¯ã€äº§å“ç­–ç•¥ã€å•†ä¸šæ¨¡å¼æœ‰å®è´¨æ€§æ·±åº¦åˆ†æçš„å†…å®¹
4. æ¥è‡ªç”¨æˆ·å¸¸çœ‹é¢‘é“çš„é«˜è´¨é‡å†…å®¹

å¿…é¡»æ’é™¤ï¼ˆå³ä½¿æ’­æ”¾é‡é«˜ä¹Ÿä¸é€‰ï¼‰ï¼š
- çº¯æ–°é—»æ±‡æ€»/é€ŸæŠ¥ç±»ï¼ˆ"AI News", "XX is HERE", "XX is INSANE" ç­‰æ ‡é¢˜å…šï¼‰
- å…¥é—¨æ•™ç¨‹/å…¨è¯¾ç¨‹ï¼ˆ"Full Course", "Tutorial For Beginners", "ä»é›¶å¼€å§‹"ï¼‰
- ä¸ AI/ç§‘æŠ€è¡Œä¸šæ— å…³çš„å†…å®¹ï¼ˆæƒ…æ„Ÿã€å¥èº«ã€çƒ¹é¥ªç­‰ï¼‰
- æ’­æ”¾é‡æä½ï¼ˆ<200ï¼‰ä¸”é¢‘é“ä¸åœ¨ç”¨æˆ·å¸¸çœ‹åˆ—è¡¨ä¸­çš„è§†é¢‘

æ’­æ”¾é‡å‚è€ƒè§„åˆ™ï¼šåŒç±»æ·±åº¦å†…å®¹ä¸­æ’­æ”¾é‡æ˜æ˜¾æ›´é«˜çš„ä¼˜å…ˆï¼Œä½†ç»ä¸å› ä¸ºæ’­æ”¾é‡é«˜å°±é€‰æ–°é—»é€ŸæŠ¥ã€‚

è¯·æŒ‰æ¨èåº¦ä»é«˜åˆ°ä½è¾“å‡ºï¼Œæ¯è¡Œä¸€ä¸ªï¼Œæ ¼å¼ä¸ºï¼š
ç¼–å·|ä¸€å¥è¯æ¨èç†ç”±

ä¾‹å¦‚ï¼š
3|Meta AI ç ”ç©¶è´Ÿè´£äººçš„ä¸€æ‰‹è§‚ç‚¹ï¼Œè®¨è®º AI è®°å¿†å’Œè§„åˆ’çš„å‰æ²¿æ–¹å‘
7|a16z æ·±åº¦è®¿è°ˆï¼Œæ­ç¤º ElevenLabs ä» 0 åˆ° 110 äº¿ç¾å…ƒçš„å¢é•¿ç­–ç•¥
1|YC åœ†æ¡Œè®¨è®º Claude Code çš„å®é™…ä½¿ç”¨ä½“éªŒå’Œå¼€å‘è€…å·¥ä½œæµå˜åŒ–

åªè¾“å‡º {top_n} è¡Œï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

    result = call_gemini(prompt)
    if not result:
        print("  âš ï¸ Gemini æ’åºå¤±è´¥ï¼Œå°è¯• MiniMax...")
        result = call_llm(prompt, max_tokens=500)
    if not result:
        print("  âš ï¸ LLM æ’åºå…¨éƒ¨å¤±è´¥ï¼Œå›é€€åˆ°æ’­æ”¾é‡æ’åº")
        candidates.sort(key=lambda v: v["view_count"], reverse=True)
        return [{"index": i, "reason": ""} for i in range(min(top_n, len(candidates)))]

    # è§£æ LLM è¿”å›çš„ "ç¼–å·|ç†ç”±" æ ¼å¼
    results = []
    for line in result.strip().splitlines():
        line = line.strip()
        if not line:
            continue
        parts = line.split("|", 1)
        nums = re.findall(r'\d+', parts[0])
        if not nums:
            continue
        idx = int(nums[0]) - 1
        reason = parts[1].strip() if len(parts) > 1 else ""
        if 0 <= idx < len(candidates) and idx not in [r["index"] for r in results]:
            results.append({"index": idx, "reason": reason})
        if len(results) >= top_n:
            break

    if not results:
        print("  âš ï¸ LLM è¿”å›è§£æå¤±è´¥ï¼Œå›é€€åˆ°æ’­æ”¾é‡æ’åº")
        candidates.sort(key=lambda v: v["view_count"], reverse=True)
        return [{"index": i, "reason": ""} for i in range(min(top_n, len(candidates)))]

    return results


# ============ é£ä¹¦æ¨é€ ============
def get_tenant_access_token() -> str:
    """è·å–é£ä¹¦ tenant_access_token"""
    if not FEISHU_APP_ID or not FEISHU_APP_SECRET:
        return ""

    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    payload = {
        "app_id": FEISHU_APP_ID,
        "app_secret": FEISHU_APP_SECRET
    }
    try:
        resp = requests.post(url, json=payload, timeout=10)
        data = resp.json()
        if data.get("code") == 0:
            return data["tenant_access_token"]
        else:
            print(f"  âš ï¸ è·å– token å¤±è´¥: {data}")
            return ""
    except Exception as e:
        print(f"  âš ï¸ è·å– token å¼‚å¸¸: {e}")
        return ""


def format_view_count(count: int) -> str:
    if count >= 1_000_000:
        return f"{count / 1_000_000:.1f}M"
    if count >= 1_000:
        return f"{count / 1_000:.1f}K"
    return str(count)


def build_card_content(videos_with_summaries: list[dict]) -> dict:
    """æ„å»ºé£ä¹¦å¡ç‰‡æ¶ˆæ¯å†…å®¹ï¼Œè¿”å›å¡ç‰‡ JSON ç»“æ„"""
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    elements = []

    for i, item in enumerate(videos_with_summaries, 1):
        v = item["video"]
        summary = item["summary"]
        view_str = format_view_count(v["view_count"])

        elements.append({"tag": "hr"})
        elements.append({"tag": "markdown", "content": f"**#{i} {v['title']}**"})
        elements.append({"tag": "note", "elements": [
            {"tag": "plain_text", "content": f"ğŸ“º {v['author']} Â· â± {v['duration_str']} Â· ğŸ‘€ {view_str} views"}
        ]})
        reason = v.get("reason", "")
        if reason:
            elements.append({"tag": "markdown", "content": f"ğŸ’¡ {reason}"})
        elements.append({"tag": "markdown", "content": summary})
        elements.append({"tag": "action", "actions": [{
            "tag": "button",
            "text": {"tag": "plain_text", "content": "â–¶ è§‚çœ‹è§†é¢‘"},
            "type": "primary",
            "url": v["url"]
        }]})

    return {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": f"ğŸ“¹ YouTube ä»Šæ—¥æ¨è ({today})"},
            "template": "blue"
        },
        "elements": elements
    }


def send_digest_to_feishu(videos_with_summaries: list[dict]):
    """å‘é€åˆå¹¶çš„æ—¥æŠ¥æ¶ˆæ¯åˆ°é£ä¹¦ï¼ˆå•æ¡æ¨é€ï¼‰"""
    if not FEISHU_APP_ID or not FEISHU_APP_SECRET or not FEISHU_USER_ID:
        print("  âš ï¸ æœªé…ç½®é£ä¹¦åº”ç”¨å‡­è¯ (FEISHU_APP_ID/SECRET/USER_ID)")
        for item in videos_with_summaries:
            print(f"  ğŸ“ {item['video']['title']}\n{item['summary']}\n")
        return

    token = get_tenant_access_token()
    if not token:
        print("  âŒ æ— æ³•è·å–é£ä¹¦ access token")
        return

    card = build_card_content(videos_with_summaries)

    body = {
        "receive_id": FEISHU_USER_ID,
        "msg_type": "interactive",
        "content": json.dumps(card)
    }

    url = "https://open.feishu.cn/open-apis/im/v1/messages?receive_id_type=user_id"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }

    try:
        resp = requests.post(url, headers=headers, json=body, timeout=10)
        result = resp.json()
        if result.get("code") == 0:
            print(f"  âœ… é£ä¹¦ä¸ªäººæ¨é€æˆåŠŸ ({len(videos_with_summaries)} ä¸ªè§†é¢‘)")
        else:
            print(f"  âŒ é£ä¹¦ä¸ªäººæ¨é€å¤±è´¥: {result}")
    except Exception as e:
        print(f"  âŒ é£ä¹¦ä¸ªäººæ¨é€å¼‚å¸¸: {e}")


def send_digest_to_webhook(videos_with_summaries: list[dict]):
    """é€šè¿‡ Webhook å‘é€æ—¥æŠ¥åˆ°é£ä¹¦ç¾¤"""
    if not FEISHU_WEBHOOK_URL:
        return

    body = {
        "msg_type": "interactive",
        "card": build_card_content(videos_with_summaries)
    }

    try:
        resp = requests.post(FEISHU_WEBHOOK_URL, json=body, timeout=10)
        result = resp.json()
        if result.get("StatusCode") == 0:
            print(f"  âœ… é£ä¹¦ç¾¤ Webhook æ¨é€æˆåŠŸ ({len(videos_with_summaries)} ä¸ªè§†é¢‘)")
        else:
            print(f"  âŒ é£ä¹¦ç¾¤ Webhook æ¨é€å¤±è´¥: {result}")
    except Exception as e:
        print(f"  âŒ é£ä¹¦ç¾¤ Webhook æ¨é€å¼‚å¸¸: {e}")


# ============ ä¸»æµç¨‹ ============
def main():
    print(f"ğŸš€ YouTube Digest å¯åŠ¨ - {datetime.now(timezone.utc).isoformat()}")
    print(f"   è¿‡æ»¤: é Shorts (>{MIN_DURATION_MINUTES}min), æœ€è¿‘ {LOOKBACK_HOURS}h, Top {TOP_N}\n")

    channels = load_channels()
    if not channels:
        print("âŒ æ— é¢‘é“é…ç½®ï¼Œé€€å‡º")
        return

    profile = load_profile()
    history = load_history()
    now_iso = datetime.now(timezone.utc).isoformat()

    # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶å‘æ‹‰å–æ‰€æœ‰é¢‘é“ RSS
    print(f"ğŸ“¡ å¹¶å‘æ‹‰å– {len(channels)} ä¸ªé¢‘é“ RSS...")
    all_rss_videos = {}  # channel_id â†’ videos
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_ch = {
            executor.submit(fetch_rss_videos, ch["channel_id"]): ch
            for ch in channels
        }
        for future in as_completed(future_to_ch):
            ch = future_to_ch[future]
            try:
                videos = future.result()
                if videos:
                    all_rss_videos[ch["channel_id"]] = videos
            except Exception as e:
                print(f"  âš ï¸ {ch.get('name', ch['channel_id'])}: {e}")

    total_rss = sum(len(v) for v in all_rss_videos.values())
    print(f"   å…±å‘ç° {total_rss} ä¸ªæ–°è§†é¢‘ï¼ˆæ¥è‡ª {len(all_rss_videos)} ä¸ªé¢‘é“ï¼‰\n")

    # æ”¶é›†å€™é€‰é•¿è§†é¢‘ï¼ˆå¸¦ quota ä¿æŠ¤ï¼‰
    candidates = []
    api_calls = 0
    API_QUOTA_LIMIT = 3000  # ä¿å®ˆé™åˆ¶ï¼Œç•™ä½™é‡ï¼ˆæ¯æ¬¡è°ƒç”¨æ¶ˆè€— 3 quotaï¼‰

    for ch in channels:
        channel_id = ch["channel_id"]
        videos = all_rss_videos.get(channel_id, [])
        if not videos:
            continue

        for video in videos:
            vid = video["video_id"]
            if vid in history:
                continue

            if api_calls >= API_QUOTA_LIMIT:
                print(f"  âš ï¸ YouTube API quota æ¥è¿‘ä¸Šé™ ({api_calls} calls)ï¼Œåœæ­¢è·å–è¯¦æƒ…")
                break

            details = get_video_details(vid)
            api_calls += 1
            duration_sec = details["duration"]
            if duration_sec < MIN_DURATION_MINUTES * 60:
                history[vid] = now_iso
                continue

            video["duration_sec"] = duration_sec
            video["duration_str"] = format_duration(duration_sec)
            video["description"] = details["description"]
            video["view_count"] = details["view_count"]
            candidates.append(video)
            print(f"   ğŸ¬ å€™é€‰: {video['title']} ({video['duration_str']}, {format_view_count(video['view_count'])} views)")

    if not candidates:
        print("\nğŸ“­ æ²¡æœ‰æ–°çš„é•¿è§†é¢‘å€™é€‰")
        save_history(history)
        return

    # ç¬¬äºŒé˜¶æ®µï¼šé¢„è¿‡æ»¤ + LLM æ™ºèƒ½ç­›é€‰
    # ç¡¬è§„åˆ™é¢„è¿‡æ»¤ï¼šå‰”é™¤æ˜æ˜¾ä¸ç¬¦åˆçš„å€™é€‰
    preferred_channels = set(profile.get("preferred_channels", []))
    exclude_patterns = profile.get("exclude_title_patterns", [])
    exclude_re = re.compile(
        r"(?i)(" + "|".join(re.escape(p) for p in exclude_patterns) + ")"
    ) if exclude_patterns else None

    filtered = []
    for v in candidates:
        # æ’é™¤å…¥é—¨æ•™ç¨‹/å…¨è¯¾ç¨‹
        if exclude_re and exclude_re.search(v["title"]):
            print(f"   â›” é¢„è¿‡æ»¤ï¼ˆæ•™ç¨‹ï¼‰: {v['title']}")
            continue
        # æ’­æ”¾é‡æä½ä¸”ä¸æ˜¯å¸¸çœ‹é¢‘é“ â†’ æ’é™¤
        is_preferred = any(pc.lower() in v["author"].lower() for pc in preferred_channels)
        if v["view_count"] < 200 and not is_preferred:
            print(f"   â›” é¢„è¿‡æ»¤ï¼ˆä½æ’­æ”¾é‡éå¸¸çœ‹é¢‘é“ï¼‰: {v['title']} ({format_view_count(v['view_count'])} views)")
            continue
        filtered.append(v)

    if not filtered:
        print("\nğŸ“­ é¢„è¿‡æ»¤åæ²¡æœ‰å€™é€‰è§†é¢‘")
        save_history(history)
        return

    if len(filtered) < len(candidates):
        print(f"   ğŸ“‹ é¢„è¿‡æ»¤: {len(candidates)} â†’ {len(filtered)} ä¸ªå€™é€‰")

    print(f"\nğŸ¤– LLM æ­£åœ¨ä» {len(filtered)} ä¸ªå€™é€‰ä¸­ç­›é€‰ Top {TOP_N}...")
    ranked = rank_candidates(filtered, TOP_N, profile)
    top_videos = [filtered[r["index"]] for r in ranked]
    # æŠŠæ¨èç†ç”±æŒ‚åˆ° video ä¸Š
    for r, v in zip(ranked, top_videos):
        v["reason"] = r["reason"]
    print(f"\nğŸ† LLM æ¨è Top {len(top_videos)}:")
    for i, v in enumerate(top_videos, 1):
        reason = f" â†’ {v['reason']}" if v.get("reason") else ""
        print(f"   {i}. [{v['author']}] {v['title']} ({v['duration_str']}, {format_view_count(v['view_count'])} views){reason}")

    # ç¬¬ä¸‰é˜¶æ®µï¼šç”Ÿæˆæ‘˜è¦ + åˆå¹¶æ¨é€
    videos_with_summaries = []
    for video in top_videos:
        # æ‘˜è¦ä¼˜å…ˆç”¨å­—å¹•ï¼ˆå†…å®¹æœ€å®Œæ•´ï¼‰ï¼Œfallback åˆ° description
        print(f"   ğŸ“ ç”Ÿæˆæ‘˜è¦: {video['title']}")
        transcript = get_transcript(video["video_id"])
        if transcript:
            result = summarize_with_llm(video["title"], video["author"], transcript, "å­—å¹•")
            summary_text = result["summary"]
        elif video["description"] and len(video["description"]) > 50:
            print(f"      âš ï¸ æ— å­—å¹•ï¼Œä½¿ç”¨ description")
            result = summarize_with_llm(video["title"], video["author"], video["description"], "æè¿°")
            summary_text = result["summary"]
        else:
            summary_text = "âš ï¸ æ— å­—å¹•ä¸”æè¿°ä¿¡æ¯ä¸è¶³ï¼Œè¯·ç›´æ¥è§‚çœ‹"

        videos_with_summaries.append({"video": video, "summary": summary_text})
        history[video["video_id"]] = now_iso
        time.sleep(1)

    # åˆå¹¶ä¸ºä¸€æ¡æ—¥æŠ¥æ¨é€
    send_digest_to_feishu(videos_with_summaries)
    send_digest_to_webhook(videos_with_summaries)

    # æœªå…¥é€‰çš„ä¹Ÿæ ‡è®°ä¸ºå·²å¤„ç†
    for video in candidates:
        history[video["video_id"]] = now_iso

    save_history(history)
    print(f"\nâœ… å®Œæˆï¼Œå…±æ¨é€ {len(top_videos)} ä¸ªè§†é¢‘ï¼ˆå€™é€‰ {len(candidates)} ä¸ªï¼ŒAPI è°ƒç”¨ {api_calls} æ¬¡ï¼‰")


if __name__ == "__main__":
    main()
